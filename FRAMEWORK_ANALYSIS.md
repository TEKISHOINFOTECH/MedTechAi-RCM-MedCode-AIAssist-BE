# ğŸ” Framework Analysis - MedTechAI RCM

## Question: Has it used Google ADK or LangChain?

**Short Answer**: 
- âŒ **Google ADK (Genkit)**: NOT used
- âš ï¸ **LangChain**: Installed but **NOT actively used** (minimal/no actual usage)

---

## ğŸ“¦ What's Currently Installed

### Dependencies in `pyproject.toml`:

```toml
[dependencies]
# AI/ML Frameworks
"openai>=1.0.0"                    âœ… ACTIVELY USED
"anthropic>=0.25.0"                âœ… CONFIGURED
"langchain>=0.1.0"                 âš ï¸ INSTALLED, NOT USED
"langchain-community>=0.0.20"      âš ï¸ INSTALLED, NOT USED
"langgraph>=0.0.20"                âš ï¸ INSTALLED, NOT USED
"google-generativeai>=0.6.0"       âœ… CONFIGURED

# Vector Store / RAG
"chromadb>=0.5.0"                  âœ… ACTIVELY USED
"tiktoken>=0.5.2"                  âœ… ACTIVELY USED
```

---

## ğŸ”¬ Actual Framework Usage

### 1. âŒ Google ADK (Genkit) - NOT USED

**What is Google ADK?**
- Google's Agentic Development Kit
- Framework for building AI agents with observability
- TypeScript/JavaScript first, Python support limited

**Status in Project**:
- âŒ Not installed
- âŒ Not imported anywhere
- âŒ No actual usage

**Why the comment says "ADK-compatible"?**
```toml
# Google Generative AI (ADK-compatible stub)
"google-generativeai>=0.6.0"
```
- This is just `google-generativeai` library (Gemini API)
- NOT the full ADK/Genkit framework
- Comment is misleading - it's just a stub for Google's LLM

**Verdict**: **Google ADK is NOT used**

---

### 2. âš ï¸ LangChain - INSTALLED BUT NOT USED

**What is LangChain?**
- Framework for building LLM applications
- Provides chains, agents, memory, tools
- Popular for RAG and multi-step workflows

**Status in Project**:
- âœ… Installed in dependencies
- âŒ **NOT imported in any Python files**
- âŒ **NOT actively used**

**Evidence**:
```bash
# Search results show:
grep -r "from langchain" app/  # NO RESULTS
grep -r "import langchain" app/  # NO RESULTS
```

**What IS being used instead?**
- **Custom LLM Client** (`app/utils/llm.py`)
- **Direct API calls** to OpenAI, Anthropic, Google
- **Custom agents** (not LangChain agents)
- **ChromaDB** directly (not via LangChain)

**Verdict**: **LangChain is installed but NOT used in actual code**

---

### 3. âš ï¸ LangGraph - INSTALLED BUT NOT USED

**What is LangGraph?**
- Extension of LangChain for building stateful agents
- Graph-based workflow orchestration
- Cyclic agent flows

**Status in Project**:
- âœ… Installed in dependencies
- âŒ NOT imported anywhere
- âŒ NOT actively used

**What IS being used instead?**
- **Custom Orchestrators**:
  - `UseCase1Pipeline` - Simple sequential pipeline
  - `EnhancedMedicalCodingOrchestrator` - Parallel/sequential execution

**Verdict**: **LangGraph is NOT used**

---

## ğŸ—ï¸ What IS Actually Being Used

### Architecture Choice: **Custom Implementation**

The project uses a **custom-built agentic framework** instead of LangChain/ADK:

#### 1. **Custom LLM Client** (`app/utils/llm.py`)

```python
class LLMClient:
    """Unified asynchronous LLM client"""
    
    def __init__(self, provider: str, model: str):
        if provider == "openai":
            self._client = AsyncOpenAI()
        elif provider == "anthropic":
            self._client = AsyncAnthropic()
        elif provider == "google":
            self._client = genai  # Google Generative AI
    
    async def chat(self, messages, temperature, max_tokens):
        # Direct API calls to LLM providers
```

**Why custom?**
- âœ… Simpler, less overhead
- âœ… Full control over API calls
- âœ… Direct provider integration
- âœ… No framework lock-in

#### 2. **Custom Agent System**

**Base Agent** (`app/agents/base_agent.py`):
```python
class BaseAgent(ABC):
    @abstractmethod
    async def process(self, input_data, **kwargs):
        pass
    
    @abstractmethod
    def validate_input(self, input_data):
        pass
```

**Concrete Agents**:
- `ParserAgent` - Document parsing
- `NoteToICDAgent` - Extract ICD codes
- `ICDToCPTAgent` - Suggest CPT codes
- `CodeValidationAgent` - Validate codes
- `SummarizerAgent` - Create summaries
- `MedicalCodeValidationAgent` - Comprehensive validation
- `ClaimProcessingAgent` - Claim analysis
- `DenialAnalysisAgent` - Denial analysis

**Why custom agents?**
- âœ… Domain-specific (medical coding)
- âœ… Simple, maintainable
- âœ… No LangChain complexity
- âœ… Direct LLM integration

#### 3. **Custom Orchestration**

**Simple Pipeline** (`app/services/pipeline/usecase1_pipeline.py`):
```python
class UseCase1Pipeline:
    async def run(self, csv_path, manual_codes):
        # 1. Parse
        parsed = await self.parser.process(...)
        
        # 2. Extract ICD
        icd = await self.note_to_icd.process(...)
        
        # 3. Suggest CPT
        cpt = await self.icd_to_cpt.process(...)
        
        # 4. Validate
        validation = await self.validator.process(...)
        
        # 5. Summarize
        summary = await self.summarizer.process(...)
        
        return {parsed, icd, cpt, validation, summary}
```

**Enhanced Orchestrator** (`app/services/orchestrator/enhanced_orchestrator.py`):
- Parallel agent execution
- RAG integration
- Confidence thresholds
- Decision engine

**Why custom orchestration?**
- âœ… Full control over flow
- âœ… Can do parallel execution
- âœ… Domain-specific logic
- âœ… No LangChain/LangGraph needed

#### 4. **Direct RAG Implementation**

**ChromaDB Integration** (`app/services/rag/chroma_store.py`):
```python
import chromadb
from chromadb.utils import embedding_functions

class MedicalRAGStore:
    def __init__(self):
        self.client = chromadb.PersistentClient(path=settings.vector_db_path)
        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
            api_key=settings.openai_api_key,
            model_name="text-embedding-3-large"
        )
    
    def add_documents(self, docs):
        # Direct ChromaDB usage
        
    def search(self, query, k=5):
        # Direct semantic search
```

**Why NOT LangChain's vector stores?**
- âœ… Direct ChromaDB is simpler
- âœ… Better performance control
- âœ… Fewer dependencies
- âœ… No abstraction overhead

---

## ğŸ“Š Framework Comparison

| Feature | LangChain Way | Current Implementation | Status |
|---------|---------------|------------------------|--------|
| **LLM Calls** | `ChatOpenAI()` chain | Direct `AsyncOpenAI()` | âœ… Custom |
| **Agents** | `initialize_agent()` | Custom `BaseAgent` | âœ… Custom |
| **Orchestration** | LangGraph | Custom pipelines | âœ… Custom |
| **Vector Store** | `Chroma()` wrapper | Direct ChromaDB | âœ… Custom |
| **Embeddings** | `OpenAIEmbeddings()` | Direct OpenAI API | âœ… Custom |
| **Prompt Templates** | `PromptTemplate` | Python f-strings | âœ… Custom |
| **Memory** | `ConversationBufferMemory` | Not needed | N/A |

---

## ğŸ¤” Why NOT LangChain/ADK?

### Advantages of Custom Implementation:

1. **Simplicity**
   - âœ… No complex framework abstractions
   - âœ… Easy to understand code
   - âœ… Direct API calls

2. **Performance**
   - âœ… Less overhead
   - âœ… Faster execution
   - âœ… Better control

3. **Flexibility**
   - âœ… Custom agent logic
   - âœ… Domain-specific features
   - âœ… Medical coding optimizations

4. **Maintainability**
   - âœ… No framework version conflicts
   - âœ… Easier debugging
   - âœ… Full code control

### When LangChain WOULD be useful:

- âŒ Complex multi-step chains (not needed here)
- âŒ Conversation memory (not needed)
- âŒ Tool calling (not heavily used)
- âŒ Document loaders (custom parsing is better)

**Verdict**: **Custom implementation is the right choice for this use case**

---

## ğŸ”„ Should We Add LangChain/ADK?

### Option 1: Keep Current (Recommended âœ…)

**Pros**:
- âœ… Already working well
- âœ… Simpler codebase
- âœ… Better performance
- âœ… Easier to maintain
- âœ… No migration needed

**Cons**:
- âŒ Missing some LangChain features (not critical)
- âŒ Manual implementation of chains

**Recommendation**: **Keep current custom implementation**

### Option 2: Migrate to LangChain

**Pros**:
- âœ… Rich ecosystem
- âœ… Pre-built components
- âœ… Community support
- âœ… Advanced features (if needed later)

**Cons**:
- âŒ Significant refactoring
- âŒ Performance overhead
- âŒ More complex codebase
- âŒ Framework lock-in
- âŒ Time consuming (~2-3 days work)

**Recommendation**: **NOT worth it for this project**

### Option 3: Hybrid Approach

Use LangChain for specific features:
- LangChain for document loading
- LangChain for advanced chains (if needed)
- Keep custom agents
- Keep custom LLM client

**Recommendation**: **Only if specific LangChain features are needed**

---

## ğŸ“ Dependencies Cleanup

### Current State:

```toml
# These are installed but NOT used:
"langchain>=0.1.0"                 # âš ï¸ Remove?
"langchain-community>=0.0.20"      # âš ï¸ Remove?
"langgraph>=0.0.20"                # âš ï¸ Remove?
```

### Recommendation:

#### Option A: Remove (Clean up)
```bash
# Remove from pyproject.toml
uv remove langchain langchain-community langgraph
```

**Pros**: Smaller dependencies, faster installs  
**Cons**: Need to reinstall if needed later

#### Option B: Keep (Future-ready)
- Keep them in case we need LangChain features later
- Minimal cost (just disk space)
- Ready to use if requirements change

**Recommended**: **Option B - Keep for now** (already installed)

---

## ğŸ¯ Summary

### Framework Status:

| Framework | Installed? | Used? | Purpose |
|-----------|-----------|-------|---------|
| **Google ADK** | âŒ No | âŒ No | N/A |
| **Google Generative AI** | âœ… Yes | âœ… Yes | Gemini LLM access |
| **LangChain** | âœ… Yes | âŒ No | Not actively used |
| **LangGraph** | âœ… Yes | âŒ No | Not actively used |
| **OpenAI SDK** | âœ… Yes | âœ… Yes | **Primary LLM** |
| **Anthropic SDK** | âœ… Yes | âœ… Yes | Claude support |
| **ChromaDB** | âœ… Yes | âœ… Yes | **Vector store** |
| **FastAPI** | âœ… Yes | âœ… Yes | **Web framework** |

### Architecture Choice:

**ğŸ† Custom Implementation** (Not LangChain, Not Google ADK)

```
Custom LLM Client
    â†“
Custom Agents (BaseAgent)
    â†“
Custom Orchestration (Pipelines)
    â†“
Direct ChromaDB Integration
    â†“
FastAPI for APIs
```

**Why?**
- âœ… Simpler and cleaner
- âœ… Better performance
- âœ… Full control
- âœ… Domain-optimized for medical coding
- âœ… Easier to maintain

---

## ğŸ”® Future Considerations

### When to Consider LangChain:

1. **Complex Chains** - If you need multi-step reasoning chains
2. **Conversation Memory** - If adding chat/conversation features
3. **Tool Calling** - If agents need to call external tools
4. **Advanced RAG** - If implementing complex RAG patterns

### When to Consider Google ADK:

1. **Enterprise Observability** - If need Google Cloud integration
2. **TypeScript Backend** - If moving to Node.js/TypeScript
3. **Google Services** - If heavily using Google Cloud

### Current Recommendation:

**âœ… Stay with custom implementation**
- Already working well
- Optimized for medical coding
- No migration needed
- Can always add frameworks later if needed

---

## ğŸ’¡ Key Takeaways

1. **NOT using Google ADK** - Only using `google-generativeai` for Gemini API
2. **NOT actively using LangChain** - Installed but not imported/used
3. **Using custom agentic framework** - Built specifically for medical coding
4. **Direct LLM integration** - OpenAI, Anthropic, Google APIs directly
5. **Custom orchestration** - Tailored pipelines for the use case

**Bottom Line**: This is a **custom-built agentic system**, not using LangChain or Google ADK frameworks.

---

**Created**: October 2, 2024  
**Last Updated**: October 2, 2024  
**Version**: 1.0


